{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class web_scraper():\n",
    "    def webScrapingg(self, url):\n",
    "        # Add header and  url\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36\"\n",
    "        }\n",
    "        # url = \"https://en.wikipedia.org/wiki/List_of_national_capitals\"\n",
    "        r = requests.get(url)\n",
    "\n",
    "        no_of_pages= 10\n",
    "        pages = []\n",
    "\n",
    "        #looping through the required pages and selecting the pages accordingly\n",
    "        for i in range(1,no_of_pages+1): #to include the last page\n",
    "            url=(f'https://etendering.ted.europa.eu/cft/cft-search.html?_caList=1&procedureTypeForthcoming=&startDateTo=&_procedureTypeForthcoming=1&confirm=Search&closingDateFrom=&closingDateTo=&startDateFrom=&_procedureTypeOngoing=1&maxResults=50&procedureTypeOngoing=&page={i}&text=&status=')\n",
    "            pages.append(url)\n",
    "\n",
    "        # Initiate beautiful and list element to extract all the rows in the table\n",
    "        table = []\n",
    "\n",
    "        for item in pages:\n",
    "            page=requests.get(item)\n",
    "            soup=BeautifulSoup(page.text,'html.parser')\n",
    "            soup.prettify()\n",
    "            rows = soup.select('tbody tr')\n",
    "\n",
    "            for row in rows:\n",
    "                table.append([t.text.strip() for t in row.select('td')])\n",
    "    \n",
    "        df = pd.DataFrame(table)\n",
    "        df.columns = [\"No.\",\"Tender reference number\", \"Title\", \"Contracting authority\", \"Status\", \"Start date\", \"End date\"]\n",
    "        df.drop(\"No.\", axis=1, inplace=True)\n",
    "        writer = pd.ExcelWriter(\"Result.xlsx\", engine='xlsxwriter')\n",
    "        df.to_excel(writer, sheet_name='Sheet1')\n",
    "        writer.save()\n",
    "        print(\"DataFrame is exported successfully to 'ResultResult.xlsx' Excel File.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame is exported successfully to 'Result.xlsx' Excel File.\n"
     ]
    }
   ],
   "source": [
    "Ans = web_scraper()\n",
    "url = \"https://etendering.ted.europa.eu/cft/cft-search.html?text=&_caList=1&status=&startDateFrom=&startDateTo=&closingDateFrom=&closingDateTo=&procedureTypeOngoing=&_procedureTypeOngoing=1&procedureTypeForthcoming=&_procedureTypeForthcoming=1&confirm=Search#\"\n",
    "Ans.webScrapingg(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71d289fd27a1fe55770209f57b4946153f7a589d2b3bb1d40babe58180772666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
